\documentclass[12pt,oneside]{book}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[none]{hyphenat}
\usepackage{geometry}
\usepackage{pdfpages}
\usepackage{graphicx}
\usepackage[titles]{tocloft}
\graphicspath{{images/}}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{setspace}
\usepackage{algorithm}
\usepackage{float}
\usepackage{algorithmic}
\usepackage{enumitem}
\usepackage{natbib}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear,open={(},close={)}}
\usepackage[hidelinks]{hyperref}
\usepackage[printonlyused]{acronym}

\begin{document}
%	\renewcommand{\arraystretch}{1.5}
%	\renewcommand{\baselinestretch}{0}
	\frontmatter
	
	%\input{titlepage}
		
	\tableofcontents
	
	\listofalgorithms
	
	\addcontentsline{toc}{chapter}{LIST OF ABBREVIATIONS}
	\chapter*{List of Abbreviations}
	\input{abbrevs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	
	\mainmatter
	
	\chapter{INTRODUCTION}
		\begin{spacing}{1.2}
			
			An \ac{ivp} is a \ac{de} together with one or more initial values.[\citenum{ryan}][\citenum{ivpwiki}] It takes what would otherwise be an entire rainbow of possible solutions and whittles them down to one specific solution. The basic idea behind this problem is that, once you differentiate a function, you lose some information about that function, more specifically, you lose the constant. By integrating $ y'(x) $, you get a family of solutions that only differ by a constant.[\citenum{krista}]
			
			\section{Definition}
				An \ac{ivp} is a differential equation: [\citenum{ivpwiki}]
				\begin{equation}
					\begin{split} \label{eq:ivp}
						y'(t) & = f(t, y(t)) \text{ with } f:\Omega \subset \mathbb{R} \times \mathbb{R}^n \rightarrow \mathbb{R}^n \\
						(t_{0}, y_{0}) & \in \Omega, \text{ called the initial condition.}
					\end{split}
				\end{equation}
				\subsection*{Observations: }
				\begin{enumerate}
					\item The given $ f $ in (\ref{eq:ivp}) is the defining function of \ac{ivp}.
					\item A unique solution, $ y(t) $, of the (\ref{eq:ivp}) exists and it satisfies $ y(t_{0}) = y_{0} $.
				\end{enumerate}
				
				\newpage
				
			\section*{Example}
				Given $ y'(t) = 5 $ and $ y(0) = -3 $, find $ y(t) $.
				\subsection*{Solution:}
					We first integrate our $ y'(t) $, then we substitute our initial condition to determine the constant (from our integration).
					\begin{equation*}
						\begin{split}
							\int y'(t) dt & = \int 5 dt \\
							 y(t) & = 5x + c \hspace*{0.3cm} \text{ where $ c $ is the constant of integration} \\
							 \text{using } y(t = 0) & = -3 \\
							 -3 & = 5(0) + c \\
							 -3 & = c \\
							 y(t) & = 2x - 3
						\end{split}
					\end{equation*}
					\textbf{Remark: } \text{Note that with a different $ y(0) $, the solution would be different.} 
			
			
			\section{Objective}
				In real-life situations, the differential equation that models a problem is too complicated to solve exactly, therefore one of the ways which is used to solved such problems is using methods which approximates the solution of the original problem.[\citenum{burden2015numerical}] In this report, I will discuss methods that approximates solutions at certain specified timestamps. \newline
				They are: [\S\ \ref{m:eul}] The Euler's Method, [\S\ \ref{m:meul}] The Modified Euler's Method, [\S\ \ref{m:rk2}] The 2nd-Order Runge-Kutta Method, [\S\ \ref{m:rk4}] The 4th-Order Runge-Kutta Method, \newline [\S\ \ref{m:ab4e}] The Adams-Bashforth 4th-Order Explicit, [\S\ \ref{m:ab4pc}] The Adams 4th-Order Predictor Corrector, [\S\ \ref{m:rkf}] The Runge-Kutta-Fehlberg, and [\S\ \ref{m:abpcvs}] The Predictor-Corrector methods
			
		\end{spacing}
		
	
	\chapter{METHODS}
	\section{The Euler's Method} \label{m:eul}
		\begin{spacing}{1.2}
			
			%\subsection{Introduction}
			The Euler method, named after Leonhard Euler, was published in his three-volume work \textit{Institutiones Calculi Integralis} in the years 1768 to 1770, and republished in his collected works.\citep{euler1913institutiones} [\citenum{butcher2016numerical}] The Euler method is a first-order numerical procedure for solving \ac{ode} with a given initial value. It is the most basic explicit method for numerical integration of \ac{ode} and is the simplest Rungeâ€“Kutta method.[\citenum{eulwiki}] \newline
			The fundamental idea of the method is based on the principle that, we can compute (or approximate) the shape of an unknown curve - in the form of a differential equation $ f(t,y) $, which starts at a given point $ y_{0} $ and time $ t_{0} $. With this information known, we can proceed to calculate the slope (tangent line) of the curve at $ y_{0} $. \newline
			The tangent line is [\citenum{eulpd}]
			\begin{equation*}
				y = y_{0} + f(t_{0}, y_{0}) \cdot (t - t_{0})
			\end{equation*}
			Now we assume that $ f(t0, y0) $ is sufficiently accurate, and thus, taking a small step along that tangent line, we can approximate the actual value of the solution, $ y_{1} $, at timestamp $ t_{1} $, using the formula:
			\begin{equation} \label{eqn:eul1}
				y_{1} = y_{0} + f(t_{0}, y_{0}) \cdot (t_{1} - t_{0})
			\end{equation}
			In general, we continue to find the next approximated solution $ y_{n+1} $ at $ t_{n+1} $, if we have the $ nth $ timestamp $ t_{n} $ and the approximation to the solution at this point, $ y_{n} $. We only need to modify (\ref{eqn:eul1}) in this manner:
			\begin{equation} \tag{2.2a}
				y_{n+1} = y_{n} + f(t_{n}, y_{n}) \cdot (t_{n+1} - t_{n})
			\end{equation}
			If we assume uniform step sizes between times, $ t $, we can define, $ h = t_{n+1} - t_{n} $. Therefore, the formula is simplified as [\citenum{eulpd}]
			\begin{equation} \label{eqn:eul} \tag{2.2b}
				y_{n+1} = y_{n} + h \cdot f(t_{n}, y_{n})
			\end{equation}
			
			\subsection*{The Truncation Errors}
				\begin{enumerate}
					\item The \textbf{\ac{lte}} of the Euler method is the error made in a single step. It is the difference between the numerical solution after one step, $y_{1}$, and the exact solution (obtained using Taylor's expansion) at time $t_{1} = t_{0} + h$.[\citenum{eulwiki}]
					\begin{equation*}
						\begin{split}
							\text{The numerical solution: } y_{1} & = y_{0} + hf(t_{0}, y_{0}) \\
							\text{The exact solution: } y(t_{0} + h) & = y(t_{0}) + hy'(t_{0}) + \frac{1}{2}h^2y^{''}(t_{0}) + O(h^{3}) \\
							\ac{lte} & = y(t_{0} + h) - y_{1} = \frac{1}{2}h^2y^{''}(t_{0}) + O(h^{3})
						\end{split}
					\end{equation*}
					
					\item The \textbf{\ac{gte}} is the error at a fixed time $t_{i}$, after however many steps the method needs to take to reach that time from the initial time. The global truncation error is the cumulative effect of the local truncation errors committed in each step.[\citenum{atkinson1991introduction}]
					\begin{equation*}
						|y(t_{i}) - y_{i}| \leq \frac{hM}{2L} (e^{L(t_{i} - t_{0})} - 1)
					\end{equation*}
					where $ M $ is an upper bound on the second derivative of y on the given interval and $ L $ is the Lipschitz constant of $ f $.[\citenum{atkinson1991introduction}]
				\end{enumerate}
			
			\subsection*{The Pseudocode}
				To approximate the solution of the \ac{ivp}
				\[ y' = f(t,y), \hspace*{0.2cm} a \leq t \leq b, \hspace*{0.2cm} y(a) = \alpha \]
				at $ N $ equally spaced numbers in the interval $ [a, b]: $ [\citenum{burden2015numerical}]
				
				\begin{algorithm}[H]
					\caption{:: Euler's Method}
					\begin{algorithmic}[1]
						\REQUIRE endpoints $ a, b $; \hspace*{0.2cm} integer $ N $; \hspace*{0.2cm} initial condition $ \alpha $
						\ENSURE approximation $ w $ to $ y $ at the $ (N + 1) $ values of $ t $
						\STATE $ h = (b - a) / N; \hspace*{0.2cm} t_0 = a; \hspace*{0.2cm} w_{0} = \alpha $
						\FOR{$ i = 0, 1, 2, \cdots, N-1$}
							\STATE $ w_{i+1} = w_{i} + hf(t_{i},w_{i}); $ \hspace*{0.5cm} \COMMENT{Compute next $ w_{i} $}
							\STATE $ t = a + ih $ \hspace*{0.5cm} \COMMENT{Compute next $ t_{i} $}
						\ENDFOR
						\RETURN $ (t, w) $
					\end{algorithmic}
				\end{algorithm}
			
		\end{spacing}

	\clearpage
	\section{The Modified Euler's Method} \label{m:meul}
		\begin{spacing}{1.2}
			
			%\subsection{Introduction}
			Euler's method is used as the foundation for Modified Euler's method. Euler's method uses the line tangent to the function at the beginning of the interval as an estimate of the slope of the function over the interval, assuming that if the step size is small, the error will be small. However, even when extremely small step sizes are used, over a large number of steps the error starts to accumulate and the estimate diverges from the actual functional value.[\citenum{heulwiki}] \newline
			The Modified Euler (which may sparingly be referred to as the Heun's method [\citenum{heulwiki}]) was developed to improve the approximated solution at $t_{i+1}$ by taking the arithmetic average of the approximated solution at the slopes $t_{i}$ and $t_{i+1}$. \newline
			The procedure for calculating the numerical solution to the (\ref{eq:ivp}) by first computing the Euler method to roughly estimate the coordinates of the next point in the solution, and then, the original estimate is recalculated using the rough estimate [\citenum{chenadvanced}]:
				\begin{equation}
					\begin{split}
						\text{rough estimate: } \tilde{y}_{i+1} & = y_{i} + hf(t_{i}, y_{i}) \\
						\text{original estimate: } y_{i+1} & = y_{i} + \frac{h}{2} \left[f(t_{i}, y_{i}) + f(t_{i+1}, \tilde{y}_{i+1}) \right]
					\end{split}
				\end{equation}
				where $ h $ is the step size an $ t_{i+1} = t_{i} + h. $
			
			\subsection*{The Truncation Errors}
				the local truncation error is $ O(h^{3}) $. The modified Euler Method is second order accurate.
			
			\clearpage
			\subsection*{The Pseudocode}
				To approximate the solution of the \ac{ivp} 
				\[ y' = f(t,y), \hspace*{0.2cm} a \leq t \leq b, \hspace*{0.2cm} y(a) = \alpha \]
				at $ N $ equally spaced numbers in the interval $ [a, b]: $
				
				\begin{algorithm}[H]
					\caption{:: Modified Euler's Method}
					\begin{algorithmic}[1]
						\REQUIRE endpoints $ a, b $; \hspace*{0.2cm} integer $ N $; \hspace*{0.2cm} initial condition $ \alpha $
						\ENSURE approximation $ w $ to $ y $ at the $ N $ values of $ t $
						\STATE $ h = (b - a) / N; \hspace*{0.2cm} t_0 = a; \hspace*{0.2cm} w_{0} = \alpha $
						\FOR{$ i = 0, 1, 2, \cdots, N-1$}
							\STATE $ \tilde{w}_{i+1} = w_{i} + hf(t_{i},w_{i}); $ \hspace*{0.5cm} \COMMENT{Compute rough (next) $ w_{i} $}
							\STATE $ w_{i+1} = w_{i} + \frac{h}{2} \left[ f(t_{i},w_{i}) + f(t_{i} + h,\tilde{w}_{i+1}) \right]; $ \hspace*{0.5cm} \COMMENT{Compute corrected (next) $ w_{i} $}
							\STATE $ t = a + ih $ \hspace*{0.5cm} \COMMENT{Compute next $ t_{i} $}
						\ENDFOR
						\RETURN $ (t, w) $
					\end{algorithmic}
				\end{algorithm}
			
		\end{spacing}
		
	\clearpage
	\section{The 2nd-Order Runge-Kutta Method} \label{m:rk2}
		\begin{spacing}{1.2}
			
			%\subsection{Introduction}
			
			
			\subsection*{The Truncation Errors}
			
			
			\subsection*{The Pseudocode}
				To approximate the solution of the \ac{ivp} 
				\[ y' = f(t,y), \hspace*{0.2cm} a \leq t \leq b, \hspace*{0.2cm} y(a) = \alpha \]
				at $ N $ equally spaced numbers in the interval $ [a, b]: $
				
				\begin{algorithm}[H]
					\caption{:: 2nd-Order Runge-Kutta Method}
					\begin{algorithmic}[1]
						\REQUIRE endpoints $ a, b $; \hspace*{0.2cm} integer $ N $; \hspace*{0.2cm} initial condition $ \alpha $
						\ENSURE approximation $ w $ to $ y $ at the $ N $ values of $ t $
						\STATE $ h = (b - a) / N; \hspace*{0.2cm} t_0 = a; \hspace*{0.2cm} w_{0} = \alpha $
						\FOR{$ i = 0, 1, 2, \cdots, N-1$}
							\STATE $ k_{1} = f(t_{i},w_{i}); $ \hspace*{0.5cm} \COMMENT{Compute $ k_{1} $}
							\STATE $ k_{2} = f\left(t_{i} + \frac{h}{2}, w_{i} + \frac{h}{2}k_{1}\right); $ \hspace*{0.5cm} \COMMENT{Compute $ k_{2} $}
							\STATE $ w_{i+1} = w_{i} + hk_{2}; $ \hspace*{0.5cm} \COMMENT{Compute next $ w_{i} $}
							\STATE $ t = a + ih $ \hspace*{0.5cm} \COMMENT{Compute next $ t_{i} $}
						\ENDFOR
						\RETURN $ (t, w) $
					\end{algorithmic}
				\end{algorithm}
			
		\end{spacing}
		
		\clearpage
	\section{The 4th-Order Runge-Kutta Method} \label{m:rk4}
		\begin{spacing}{1.2}
			
			%\subsection{Introduction}
			
			
			\subsection*{The Truncation Errors}
			
			
			\subsection*{The Pseudocode}
				To approximate the solution of the \ac{ivp} 
				\[ y' = f(t,y), \hspace*{0.2cm} a \leq t \leq b, \hspace*{0.2cm} y(a) = \alpha \]
				at $ N $ equally spaced numbers in the interval $ [a, b]: $ [\citenum{burden2015numerical}]
				
				\begin{algorithm}[H]
					\caption{:: 4th-Order Runge-Kutta Method}
					\begin{algorithmic}[1]
						\REQUIRE endpoints $ a, b $; \hspace*{0.2cm} integer $ N $; \hspace*{0.2cm} initial condition $ \alpha $
						\ENSURE approximation $ w $ to $ y $ at the $ N $ values of $ t $
						\STATE $ h = (b - a) / N; \hspace*{0.2cm} t_0 = a; \hspace*{0.2cm} w_{0} = \alpha $
						\FOR{$ i = 0, 1, 2, \cdots, N-1$}
						\STATE $ k_{1} = f(t_{i},w_{i}); $ \hspace*{0.5cm} \COMMENT{Compute $ k_{1} \; to \; k_{4} $}
						\STATE $ k_{2} = f\left(t_{i} + \frac{h}{2}, w_{i} + \frac{h}{2}k_{1}\right); $
						\STATE $ k_{3} = f\left(t_{i} + \frac{h}{2}, w_{i} + \frac{h}{2}k_{2}\right); $
						\STATE $ k_{4} = f\left(t_{i} + h, w_{i} + hk_{3}\right); $
						\STATE $ K = k_{1} + 2k_{2} + 2k_{3} + k_{4}  $
						\STATE $ w_{i+1} = w_{i} + \frac{h}{6}K; $ \hspace*{0.5cm} \COMMENT{Compute next $ w_{i} $}
						\STATE $ t = a + ih $ \hspace*{0.5cm} \COMMENT{Compute next $ t_{i} $}
						\ENDFOR
						\RETURN $ (t, w) $
					\end{algorithmic}
				\end{algorithm}
			
		\end{spacing}
		
		\clearpage
	\section{The Adams-Bashforth 4th-Order Explicit Method} \label{m:ab4e}
		\begin{spacing}{1.2}
			
			%\subsection{Introduction}
			
			
			\subsection*{The Truncation Errors}
			
			
			\subsection*{The Pseudocode}
				To approximate the solution of the \ac{ivp} 
				\[ y' = f(t,y), \hspace*{0.2cm} a \leq t \leq b, \hspace*{0.2cm} y(a) = \alpha \]
				at $ N $ equally spaced numbers in the interval $ [a, b]: $ [\citenum{burden2015numerical}]
				
%				\begin{algorithm}[H]
%					\caption{:: 4th-Order Runge-Kutta Method}
%					\begin{algorithmic}[1]
%						\REQUIRE endpoints $ a, b $; \hspace*{0.2cm} integer $ N $; \hspace*{0.2cm} initial condition $ \alpha $
%						\ENSURE approximation $ w $ to $ y $ at the $ N $ values of $ t $
%						\STATE $ h = (b - a) / N; \hspace*{0.2cm} t_0 = a; \hspace*{0.2cm} w_{0} = \alpha $
%						\FOR{$ i = 0, 1, 2, \cdots, N-1$}
%						\STATE $ k_{1} = f(t_{i},w_{i}); $ \hspace*{0.5cm} \COMMENT{Compute $ k_{1} \; to \; k_{4} $}
%						\STATE $ k_{2} = f\left(t_{i} + \frac{h}{2}, w_{i} + \frac{h}{2}k_{1}\right); $
%						\STATE $ k_{3} = f\left(t_{i} + \frac{h}{2}, w_{i} + \frac{h}{2}k_{2}\right); $
%						\STATE $ k_{4} = f\left(t_{i} + h, w_{i} + hk_{3}\right); $
%						\STATE $ K = k_{1} + 2k_{2} + 2k_{3} + k_{4}  $
%						\STATE $ w_{i+1} = w_{i} + \frac{h}{6}K; $ \hspace*{0.5cm} \COMMENT{Compute next $ w_{i} $}
%						\STATE $ t = a + ih $ \hspace*{0.5cm} \COMMENT{Compute next $ t_{i} $}
%						\ENDFOR
%						\RETURN $ (t, w) $
%					\end{algorithmic}
%				\end{algorithm}
			
		\end{spacing}
		
		\clearpage
	\section{The Adams 4th-Order Predictor-Corrector Method} \label{m:ab4pc}
		\begin{spacing}{1.2}
			
			%\subsection{Introduction}
			
			
			\subsection*{The Truncation Errors}
			
			
			\subsection*{The Pseudocode}
				To approximate the solution of the \ac{ivp} 
				\[ y' = f(t,y), \hspace*{0.2cm} a \leq t \leq b, \hspace*{0.2cm} y(a) = \alpha \]
				at $ N $ equally spaced numbers in the interval $ [a, b]: $ [\citenum{burden2015numerical}]
				
				\begin{algorithm}[H]
					\caption{:: Adams Forth-Order Predictor-Corrector}
					\begin{algorithmic}[1]
						\REQUIRE endpoints $ a, b $; \hspace*{0.2cm} integer $ N $; \hspace*{0.2cm} initial condition $ \alpha $
						\ENSURE approximation $ w $ to $ y $ at the $ N $ values of $ t $
						\STATE $ h = (b - a) / N $
						\STATE $ t_0 = a $
						\STATE $ w_0 = \alpha $
						\FOR{$ i = 0, 1, 2 $}
							\STATE $ k_1 = hf(t_{i}, w_{i}); $ \COMMENT{Compute starting values using Runge-Kutta Method}
							\STATE $ k_2 = hf(t_{i} + \frac{h}{2}, w_{i} + \frac{k_{1}}{2}); $
							\STATE $ k_3 = hf(t_{i} + \frac{h}{2}, w_{i} + \frac{k_{2}}{2}); $
							\STATE $ k_4 = hf(t_{i} + h, w_{i} + k_{3}); $
							\STATE $ K = k_{1} + 2k_{2} + 2k_{3} + k_{4} $
							\STATE $ w_{i+1} = w_{i} + \frac{K}{6}; $
							\STATE $ t_{i+1} = a + ih $
							\RETURN $ (t_{i}, w_{i}) $
						\ENDFOR
						\FOR{$ i = 3, \cdots, N-1 $}
							\STATE $ t = a + ih; $
							\STATE $ w_{i+1} = w_{3} + h \frac{[ 55f(t_{3}. w_{3}) - 59f(t_{2}, w_{2}) + 37f(t_{1}, w_{1}) - 9f(t_{0}, w_{0}) ]}{24}; $ \COMMENT{Predict $ w_{i+1} $}
							\STATE $ w_{i+1} = w_{3} + h \frac{[ 9f(t_{i+1}. w_{i+1}) - 19f(t_{3}, w_{3}) - 5f(t_{2}, w_{2}) + f(t_{1}, w_{1}) ]}{24}; $ \COMMENT{Correct $ w_{i+1} $}
							\RETURN $ (t, w) $
							\FOR{$ j = 0, 1, 2 $}
								\STATE $ t_{j} = t_{j+1}; $
								\STATE $ w_{j} = w_{j+1} $
							\ENDFOR
							\STATE $ t_{3} = t_{i+1}; $
							\STATE $ w_{3} = w_{i+1} $
						\ENDFOR
					\end{algorithmic}
				\end{algorithm}
			
		\end{spacing}
		
		\clearpage
	\section{The Runge-Kutta-Fehlberg Method} \label{m:rkf}
		\begin{spacing}{1.2}
			
			%\subsection{Introduction}
			
			
			\subsection*{The Truncation Errors}
			
			
			\subsection*{The Pseudocode} 
				To approximate the solution of the \ac{ivp} 
				\[ y' = f(t,y), \hspace*{0.2cm} a \leq t \leq b, \hspace*{0.2cm} y(a) = \alpha \]
				with local truncation error within a given tolerance: [\citenum{burden2015numerical}]
				
				\begin{algorithm}[H]
					\caption{:: \ac{rkf}}
					\begin{algorithmic}[1]
						\REQUIRE endpoints $ a, b $; \hspace*{0.2cm} a tolerance $ TOL $; \hspace*{0.2cm} initial condition $ \alpha $ \hspace*{0.2cm} maximum step size $ hmax $; \hspace*{0.2cm} minimum step size $ hmin $
						\ENSURE $ t, w, h $ where $ w $ approximates $ y(t) $ and the step size $ h $ was used, or a message that the minimum step size was exceeded.
						\STATE $ t = a; \hspace*{0.2cm} w = \alpha; \hspace*{0.2cm} h = hmax; Flag=1; $
						\WHILE{$ Flag==1 $}
							\STATE $ k_1 = hf(t,w); $
							\STATE $ k_2 = hf(t + \frac{1}{4}h, w + \frac{1}{4}k_1); $
							\STATE $ k_3 = hf(t + \frac{3}{8}h, w + \frac{3}{32}k_1 + \frac{9}{32}k_2); $
							\STATE $ k_4 = hf(t + \frac{12}{13}h, w + \frac{1932}{2197}k_1 - \frac{7200}{2197}k_2 + \frac{7296}{2197}k_3); $
							\STATE $ k_5 = hf(t + h, w + \frac{439}{216}k_1 - 8k_2 + \frac{3680}{513}k_3 - \frac{845}{4104}k_4); $
							\STATE $ k_6 = hf(t + \frac{1}{2}h, w - \frac{8}{27}k_1 + 2k_2 - \frac{3544}{2565}k_3 + \frac{1859}{4104}k_4 - \frac{11}{40}k_5); $
							\STATE $ R = \frac{1}{h} | \frac{1}{360}k_1 - \frac{128}{4275}k_3 - \frac{2197}{75240}k_4 + \frac{1}{50}k_5 + \frac{2}{55}k_6 | $ \hspace*{0.5cm} \COMMENT{Note: $ R = \frac{1}{h} | \tilde{w}_{i+1} - w_{i+1} | $}
							\IF{$ R \leq TOL $}
								\STATE $ t = t + h; $
								\STATE $ w = w + \frac{25}{216}k_1 + \frac{1408}{2565}k_3 + \frac{2197}{4104}k_4 - \frac{1}{5}k_5 $
								\RETURN $ (t, w, h) $
							\ENDIF
							\STATE $ \delta = 0.84 (TOL/R)^{\frac{1}{4}} $
							\IF{$ \delta \leq 0.1 $} 
								\STATE $ h = 0.1h $\hspace*{0.5cm} \COMMENT{Calculate new $ h $}
							\ELSIF{$ \delta \geq 4 $}
								\STATE $ h = 4h $
							\ELSE
								\STATE $ h = \delta h $
							\ENDIF
							\IF{$ h > hmax $}
								\STATE $ h = hmax $
							\ENDIF
							\IF{$ t \geq b $}
								\STATE $ Flag = 0 $
							\ELSIF{$ t + h > b $}
								\STATE $ h = b - t $
							\ELSIF{$ h < hmin $}
								\STATE $ Flag = 0 $
								\PRINT "minimum $ h $ exceeded" \hspace*{0.5cm} \COMMENT{Procedure completed unsuccessfully}
							\ENDIF
						\ENDWHILE
						\RETURN $ (t, w) $
					\end{algorithmic}
				\end{algorithm}
			
		\end{spacing}
		
		\clearpage
	\section{The Adams Variable Step-Size Predictor-Corrector} \label{m:abpcvs}
		\begin{spacing}{1.2}
			
			%\subsection{Introduction}
			
			
			\subsection*{The Truncation Errors}
			
			
			\subsection*{The Pseudocode}
				To approximate the solution of the \ac{ivp} 
				\[ y' = f(t,y), \hspace*{0.2cm} a \leq t \leq b, \hspace*{0.2cm} y(a) = \alpha \]
				with local truncation error within a given tolerance: [\citenum{burden2015numerical}]
				
				\begin{algorithm}[H]
					\caption{:: Adams Variable Step-Size Predictor-Corrector}
					\begin{algorithmic}[1]
						\REQUIRE endpoints $ a, b $; \hspace*{0.2cm} a tolerance $ TOL $; \hspace*{0.2cm} initial condition $ \alpha $ \hspace*{0.2cm} maximum step size $ hmax $; \hspace*{0.2cm} minimum step size $ hmin $
						\ENSURE $ t, w, h $ where $ w $ approximates $ y(t) $ and the step size $ h $ was used, or a message that the minimum step size was exceeded.
						\STATE \COMMENT{Set up a subalgorithm for the Runge-Kutta 4th-Order method to be called $ RK4(h, v_{0}, x_{0}, v_{1}, x_{1}, v_{2}, x_{2}, v_{3}, x_{3}) $ that accepts as input a step size $ h $ and starting values $ v_{0} \approx y(t_{0}) $ and returns $ \{ (x_{j}, v_{j}) | j = 1, 2, 3 \} $ defined by the following:}
						\FOR{$ j = 1, 2, 3 $}
							\STATE $ k_{1} = hf(x_{j-1}, v_{j-1}); $
							\STATE $ k_{2} = hf(x_{j-1} + \frac{h}{2}, v_{j-1} + \frac{k_{1}}{2}); $
							\STATE $ k_{3} = hf(x_{j-1} + \frac{h}{2}, v_{j-1} + \frac{k_{2}}{2}); $
							\STATE $ k_{4} = hf(x_{j-1} + h, v_{j-1} + k_{3}); $
							\STATE $ K = k_{1} + 2k_{2} + 2k_{3} + k_{4} $
							\STATE $ v_{j} = v_{j-1} + \frac{K}{6}; $
							\STATE $ x_{j} = x_{0} + jh $
						\ENDFOR
						\STATE $ t_{0} = a; $
						\STATE $ w_{0} = \alpha; $
						\STATE $ h = hmax; $
						\STATE $ Flag = 1 $ \hspace*{0.5cm} \COMMENT{$ Flag $ will be used to exit a loop.}
						\STATE $ Last = 0 $ \hspace*{0.5cm} \COMMENT{$ Last $ will indicate when the last value is calculated.}
						\STATE Call $ RK4(h, v_{0}, x_{0}, v_{1}, x_{1}, v_{2}, x_{2}, v_{3}, x_{3}) $
						\STATE $ Nflag = 1; $ \hspace*{0.5cm} \COMMENT{Indicates computation from $ RK4 $}
						\STATE $ i = 4; $
						\STATE $ t = t_{3} + h $
						\WHILE{$ Flag == 1 $}
							\STATE $ w_{p} = w_{i-1} + \frac{h}{24} [ 55f(t_{i-1}, w_{i-1}) - 59f(t_{i-2}, w_{i-2}) + 37f(t_{i-3}, w_{i-3}) - 9f(t_{i-4}, w_{i-4}); ] $ \hspace*{0.5cm} \COMMENT{Predict $ w_{i} $}
							\STATE $ w_{c} = w_{i-1} + \frac{h}{24} [ 9f(t, w_{p}) + 19f(t_{i-1}, w_{i-1}) - 5f(t_{i-2}, w_{i-2}) + f(t_{i-3}, w_{i-3}); ] $ \hspace*{0.5cm} \COMMENT{Correct $ w_{i} $}
							\STATE $ \sigma = 19 | w_{c} - w_{p} | / 270h $
							\IF{$ \sigma \leq TOL $}
								\STATE $ w_{i} = w_{c} $ \hspace*{0.5cm} \COMMENT{Result accepted}
								\STATE $ t_{i} = t $
								\IF{$ Nflag == 1 $}
									\FOR{$ j = i-3, i-2, i-1, i $}
										\RETURN $ (j, t_{j}, w_{j}, h); $ \hspace*{0.5cm} \COMMENT{Previous results also accepted}
									\ENDFOR
								\ELSE
									\RETURN $ (i, t_{i}, w_{i}, h) $ \hspace*{0.5cm} \COMMENT{Previous results already accepted}
								\ENDIF
								\IF{$ Last == 1 $}
									\STATE $ Flag = 0 $
								\ELSE %10-16
									\STATE $ i = i + 1 $
									\STATE $ Nflag = 0 $
									\IF{$ \sigma \leq 0.1 TOL $ \OR $ t_{i-1} + h > b $ }
									\STATE $ q = (TOL / 2\sigma)^{\frac{1}{4}} $ %12
									\ENDIF
								\ENDIF
							%\ELSE % 17 - 19
							
							\ENDIF
						\ENDWHILE
					\end{algorithmic}
				\end{algorithm}
			
		\end{spacing}
		






	\chapter{NUMERICAL EXPERIMENTS}
	
	
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\backmatter
	
	\addcontentsline{toc}{chapter}{REFERENCES}
	\renewcommand{\bibname}{REFERENCES}
	\bibliography{refs}
	
\end{document}