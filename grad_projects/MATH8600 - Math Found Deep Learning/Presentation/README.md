# [SpanBERT: Improving Pre-training by Representing and Predicting Spans - A Presentation](https://youtu.be/udo19ynnMaY) <br>
**AUTHORS: Mandar Joshi∗ &nbsp; &nbsp; Danqi Chen∗ &nbsp; &nbsp; Yinhan Liu &nbsp; &nbsp; Daniel S. Weld &nbsp; &nbsp; Luke Zettlemoyer &nbsp; &nbsp; Omer Levy**

---

SpanBERT, an extension of BERT, revolutionizes pretraining by masking text spans instead of random tokens.
It enhances learning longer dependencies by discarding BERT's Next Sentence Prediction task.
Tailored for tasks involving spans, such as Extractive Question Answering, Coreference Resolution, and
Relation Extraction, SpanBERT excels in understanding contextual relationships within text. <br> 

[Link to presentation (click here)!](https://youtu.be/udo19ynnMaY)
