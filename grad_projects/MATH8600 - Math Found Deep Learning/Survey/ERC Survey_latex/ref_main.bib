@article{meld,
  title={Meld: A multimodal multi-party dataset for emotion recognition in conversations},
  author={Poria, Soujanya and Hazarika, Devamanyu and Majumder, Navonil and Naik, Gautam and Cambria, Erik and Mihalcea, Rada},
  journal={arXiv preprint arXiv:1810.02508},
  year={2018}
}

@article{iemocap,
  title={IEMOCAP: Interactive emotional dyadic motion capture database},
  author={Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh, Abe and Mower, Emily and Kim, Samuel and Chang, Jeannette N and Lee, Sungbok and Narayanan, Shrikanth S},
  journal={Language resources and evaluation},
  volume={42},
  pages={335--359},
  year={2008},
  publisher={Springer}
}

@article{emotionlines,
  title={Emotionlines: An emotion corpus of multi-party conversations},
  author={Chen, Sheng-Yeh and Hsu, Chao-Chun and Kuo, Chuan-Chun and Ku, Lun-Wei and others},
  journal={arXiv preprint arXiv:1802.08379},
  year={2018}
}

@article{semaine,
  title={The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent},
  author={McKeown, Gary and Valstar, Michel and Cowie, Roddy and Pantic, Maja and Schroder, Marc},
  journal={IEEE transactions on affective computing},
  volume={3},
  number={1},
  pages={5--17},
  year={2011},
  publisher={IEEE}
}

@article{avec2012,
  title={Avec 2012: the continuous audio/visual emotion challenge},
  author={Schuller, Bj{\"o}rn and Valster, Michel and Eyben, Florian and Cowie, Roddy and Pantic, Maja},
  journal={Proceedings of the 14th ACM international conference on Multimodal interaction},
  pages={449--456},
  year={2012}
}

@article{moud,
  title={Utterance-level multimodal sentiment analysis},
  author={P{\'e}rez-Rosas, Ver{\'o}nica and Mihalcea, Rada and Morency, Louis-Philippe},
  journal={Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={973--982},
  year={2013}
}

@article{mosi,
  title={Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages},
  author={Zadeh, Amir and Zellers, Rowan and Pincus, Eli and Morency, Louis-Philippe},
  journal={IEEE Intelligent Systems},
  volume={31},
  number={6},
  pages={82--88},
  year={2016},
  publisher={IEEE}
}

@article{alm,
  title={Emotions from text: machine learning for text-based emotion prediction},
  author={Alm, Cecilia Ovesdotter and Roth, Dan and Sproat, Richard},
  journal={Proceedings of human language technology conference and conference on empirical methods in natural language processing},
  pages={579--586},
  year={2005}
}

@article{isear,
  title={Evidence for universality and cultural variation of differential emotion response patterning.},
  author={Scherer, Klaus R and Wallbott, Harald G},
  journal={Journal of personality and social psychology},
  volume={66},
  number={2},
  pages={310},
  year={1994},
  publisher={American Psychological Association}
}

@article{ercontextsota,
  title={A survey of state-of-the-art approaches for emotion recognition in text},
  author={Alswaidan, Nourah and Menai, Mohamed El Bachir},
  journal={Knowledge and Information Systems},
  volume={62},
  pages={2937--2987},
  year={2020},
  publisher={Springer}
}

@article{cnn-text,
  title={Convolutional neural networks for sentence classification},
  author={Kim, Yoon},
  journal={arXiv preprint arXiv:1408.5882},
  year={2014}
}

@article{cmn,
  title={Conversational memory network for emotion recognition in dyadic dialogue videos},
  author={Hazarika, Devamanyu and Poria, Soujanya and Zadeh, Amir and Cambria, Erik and Morency, Louis-Philippe and Zimmermann, Roger},
  journal={Proceedings of the conference. Association for Computational Linguistics. North American Chapter. Meeting},
  volume={2018},
  pages={2122},
  year={2018},
  organization={NIH Public Access}
}

@article{dialoguernn,
  title={Dialoguernn: An attentive rnn for emotion detection in conversations},
  author={Majumder, Navonil and Poria, Soujanya and Hazarika, Devamanyu and Mihalcea, Rada and Gelbukh, Alexander and Cambria, Erik},
  journal={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={6818--6825},
  year={2019}
}

@article{bclstm,
  title={Context-dependent sentiment analysis in user-generated videos},
  author={Poria, Soujanya and Cambria, Erik and Hazarika, Devamanyu and Majumder, Navonil and Zadeh, Amir and Morency, Louis-Philippe},
  journal={Proceedings of the 55th annual meeting of the association for computational linguistics (volume 1: Long papers)},
  pages={873--883},
  year={2017}
}

@article{dialoguetrm,
  title={Dialoguetrm: Exploring the intra-and inter-modal emotional behaviors in the conversation},
  author={Mao, Yuzhao and Sun, Qi and Liu, Guang and Wang, Xiaojie and Gao, Weiguo and Li, Xuan and Shen, Jianping},
  journal={arXiv preprint arXiv:2010.07637},
  year={2020}
}

@article{hitrans,
  title={Hitrans: A transformer-based context-and speaker-sensitive model for emotion detection in conversations},
  author={Li, Jingye and Ji, Donghong and Li, Fei and Zhang, Meishan and Liu, Yijiang},
  journal={Proceedings of the 28th International Conference on Computational Linguistics},
  pages={4190--4200},
  year={2020}
}

@article{dialogxl,
  title={Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition},
  author={Shen, Weizhou and Chen, Junqing and Quan, Xiaojun and Xie, Zhixian},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={15},
  pages={13789--13797},
  year={2021}
}

@article{m2fnet,
  title={M2fnet: Multi-modal fusion network for emotion recognition in conversation},
  author={Chudasama, Vishal and Kar, Purbayan and Gudmalwar, Ashish and Shah, Nirmesh and Wasnik, Pankaj and Onoe, Naoyuki},
  journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4652--4661},
  year={2022}
}

@article{emoberta,
  title={Emoberta: Speaker-aware emotion recognition in conversation with roberta},
  author={Kim, Taewoon and Vossen, Piek},
  journal={arXiv preprint arXiv:2108.12009},
  year={2021}
}

@article{roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{fmmt,
  title={A Facial Expression-Aware Multimodal Multi-task Learning Framework for Emotion Recognition in Multi-party Conversations},
  author={Zheng, Wenjie and Yu, Jianfei and Xia, Rui and Wang, Shijin},
  journal={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={15445--15459},
  year={2023}
}

@article{multiemo,
  title={MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations},
  author={Shi, Tao and Huang, Shao-Lun},
  journal={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={14752--14766},
  year={2023}
}

@article{opensmile,
  title={Opensmile: the munich versatile and fast open-source audio feature extractor},
  author={Eyben, Florian and W{\"o}llmer, Martin and Schuller, Bj{\"o}rn},
  journal={Proceedings of the 18th ACM international conference on Multimedia},
  pages={1459--1462},
  year={2010}
}

@article{w2v,
  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={12449--12460},
  year={2020}
}

@article{tao,
  title={Is someone speaking? exploring long-term temporal features for audio-visual active speaker detection},
  author={Tao, Ruijie and Pan, Zexu and Das, Rohan Kumar and Qian, Xinyuan and Shou, Mike Zheng and Li, Haizhou},
  journal={Proceedings of the 29th ACM International Conference on Multimedia},
  pages={3927--3935},
  year={2021}
}

@article{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  journal={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{facenet,
  title={Facenet: A unified embedding for face recognition and clustering},
  author={Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
  journal={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={815--823},
  year={2015}
}

@article{trancnn,
  title={Learning spatiotemporal features with 3d convolutional networks},
  author={Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
  journal={Proceedings of the IEEE international conference on computer vision},
  pages={4489--4497},
  year={2015}
}

@article{ji3d,
  title={3D convolutional neural networks for human action recognition},
  author={Ji, Shuiwang and Xu, Wei and Yang, Ming and Yu, Kai},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={1},
  pages={221--231},
  year={2012},
  publisher={IEEE}
}

@article{mtcnn,
  title={Joint face detection and alignment using multitask cascaded convolutional networks},
  author={Zhang, Kaipeng and Zhang, Zhanpeng and Li, Zhifeng and Qiao, Yu},
  journal={IEEE signal processing letters},
  volume={23},
  number={10},
  pages={1499--1503},
  year={2016},
  publisher={IEEE}
}

@article{vggface2,
  title={Vggface2: A dataset for recognising faces across pose and age},
  author={Cao, Qiong and Shen, Li and Xie, Weidi and Parkhi, Omkar M and Zisserman, Andrew},
  journal={2018 13th IEEE international conference on automatic face \& gesture recognition (FG 2018)},
  pages={67--74},
  year={2018},
  organization={IEEE}
}

@article{rosvall,
  title={Maps of random walks on complex networks reveal community structure},
  author={Rosvall, Martin and Bergstrom, Carl T},
  journal={Proceedings of the national academy of sciences},
  volume={105},
  number={4},
  pages={1118--1123},
  year={2008},
  publisher={National Acad Sciences}
}

@article{cmt,
  title={Multimodal transformer for unaligned multimodal language sequences},
  author={Tsai, Yao-Hung Hubert and Bai, Shaojie and Liang, Paul Pu and Kolter, J Zico and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  journal={Proceedings of the conference. Association for Computational Linguistics. Meeting},
  volume={2019},
  pages={6558},
  year={2019},
  organization={NIH Public Access}
}

@article{bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{txl,
  title={Transformer-xl: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}

@online{wikidarwin,
    author    = "{Charles Robert Dawin}",
    title     = "{The expression of the emotions in man and animals}",
    year      = "{1872}",
    url       = "{http://darwin-online.org.uk/content/frameset?itemID=F1142&viewtype=text&pageseq=1}",
}

@article{ekman,
  title={Basic emotions},
  author={Ekman, Paul and others},
  journal={Handbook of cognition and emotion},
  volume={98},
  number={45-60},
  pages={16},
  year={1999}
}

@incollection{erkin,
    author = {Asutay, Erkin and Västfjäll, Daniel},
    isbn = {9780190460242},
    title = "{368Sound and Emotion}",
    booktitle = "{The Oxford Handbook of Sound and Imagination, Volume 2}",
    publisher = {Oxford University Press},
    year = {2019},
    month = {09},
    doi = {10.1093/oxfordhb/9780190460242.013.23},
    url = {https://doi.org/10.1093/oxfordhb/9780190460242.013.23},
    eprint = {https://academic.oup.com/book/0/chapter/333830406/chapter-ag-pdf/44447323/book\_38547\_section\_333830406.ag.pdf},
}

@article{emocaps,
  title={Emocaps: Emotion capsule based model for conversational emotion recognition},
  author={Li, Zaijing and Tang, Fengxiao and Zhao, Ming and Zhu, Yusen},
  journal={arXiv preprint arXiv:2203.13504},
  year={2022}
}

@article{visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2304.08485},
  year={2023}
}

@article{assistant,
  title={A general language assistant as a laboratory for alignment},
  author={Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and others},
  journal={arXiv preprint arXiv:2112.00861},
  year={2021}
}
