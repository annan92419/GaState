\section{Strengths and Weaknesses of ERC Models}

    The CNN model \cite{cnn-text} introduced by Yoon Kim possesses two notable strengths and has influenced papers such as DialogueRNN \cite{dialoguernn}. This model has the ability to automatically learn hierarchical representations of text, capturing both word-level and sentence-level features. Additionally, it utilizes filters to identify crucial phrases or word combinations contributing to classification tasks. However, it treats words as discrete entities and may face challenges in comprehending long-range dependencies crucial for certain text understanding tasks. Poria et al. \cite{bclstm} made one of the initial attempts to model \ac{erc} using multi-modal data. They consider the broader context of the video, such as tone of voice. However, incorporating surrounding scenes in emotion detection might potentially introduce uncertainty or ambiguity into the task. Hazarika et al. proposed the CMN model \cite{cmn}, effectively capturing temporal dependencies and contextual information through memory mechanisms. While it considers conversational history to understand emotions within current dialogues, its memory mechanism is limited and struggles with longer sequences. Additionally, there's a lack of information on how the memory network represents and utilizes context in dialogues. This led to the development of DialogueRNN (Majumder et al.), one of the most cited papers in \ac{erc} research. DialogueRNN incorporates an attention mechanism to focus on relevant parts of the conversation and considers the temporal context of the conversation. Despite performing well on multi-modal \ac{erc} tasks, DialogueRNN is ineffective in capturing long-range dependencies in conversations. The emergence of transformer-based models like Hi-Trans shares a common weakness in interpreting how the model makes predictions given an utterance. However, Hi-Trans shows sensitivity to different speakers within a conversation, enhancing the accuracy of detecting emotions. Shen et al. introduced DialogXL, stemming from ideas presented in XLNet and Transformer-XL. DialogXL's attention mechanisms cater to both intra-speaker (between speakers) and inter-speaker (within a speaker's history) dependencies in multi-party conversations. Its memory-based approach allows for a context-aware understanding of emotions within ongoing conversations. While novel, the incorporation of four attention mechanisms and a memory-based approach make the model complex. EmoBERTa, a simple RoBERTa-based model introduced by Kim et al., accounts for inter-speaker and intra-speaker dynamics and benefits from RoBERTa's pre-trained language representations. It performs well even with limited labeled data but is constrained to textual context. The hierarchical structure of DialogueTRM \cite{dialoguetrm} encodes both intra- and inter-speaker information and introduces a novel fusion mechanism, \ac{mgif}, for a more contextual representation of multi-modal features. Zheng et al.'s FacialMMT \cite{fmmt} model focuses on distinguishing surrounding noise from speakers and properly capturing facial expressions of the main speaker. However, for short sequences, facial features like mouth movement are hard-coded, making the model complex. Chudasama's M2FNet \cite{m2fnet} introduces a new feature extractor using an adaptive margin-based triplet loss function to learn emotion-relevant features from audio and visual data. However, it doesn't consider the temporal dynamics of the conversation. The more recent MultiEMO \cite{multiemo} by Shi et al. captures relationships among different modalities using a multi-head attention mechanism and introduces the \ac{swfc} loss function, addressing challenges in classifying minority or semantically similar emotions. Yet, the complex fusion mechanisms might hinder the interpretability of the model's decision-making process.
